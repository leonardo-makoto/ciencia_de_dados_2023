---
title: "MAE 5905: Introdução à Ciência de Dados - Lista 4" 
author:
  - "Leonardo Lima - 14334311"
  - "Leonardo Makoto - 7180679"
date: "2023-06-20" 
output: pdf_document
---

# Questão 1
Determine as componentes principais para o conjunto de dados *iris* disponível por meio do comando **`data(iris)`** no pacote R.
------------------------------------------------------------------------
```{r message=FALSE, warning=FALSE}
# Caregando os pacotes de manipulação de dados
library(tidyverse)
# vamos carregar o pacote para produção de gráfico de correlação corrplot
library(corrplot)
# carregando a base de dados
data("iris")
```
Para os dados Iris, a variável dependente dos modelos é a factor **Species**, que contém 3 categorias: setosa, versicolor e virginica. A análise de componente principal (ACP), assim como análise fatorial (AF) e Análise de Componentes Independentes (ACI) é um método que tem o objetivo de reduzir a dimensionalidade de observações multivariadas com base em sua estrutura de dependência. 

Nesse sentido, a primeira coisa a se fazer durante a aplicação do PCA é observar a correlação linear entre as variáveis explicativas do modelo que buscamos implementar:
```{r}
# criando a correlação entre as variáveis
correlacao <- cor(iris[,1:4], method = "pearson")
# paleta de cores pasteis para usar no gráfico de correlação
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# produzindo um gráfico para visualização
corrplot(correlacao, method = "color",
         type = "upper", col = col(200),
         addCoef.col = "black",
         tl.col="black", tl.srt=45)
# observações:
# 1. method = color determina o formato do gráfico, para ser quadrados coloridos;
# 2. type = upper determina que só deve aparecer a parte superior do correlograma;
# 3. col(200) determina que o espectro de cores entre 1 e -1 tenha 200 bandas;
# 4. add.coef.col faz com que o valor da correlação seja reportado
#    junto com as cores com, além de determinar a cor do número;
# 5. tl.col r tl.srt determinam, respectivamente,
#    a cor e a inclinação do nome dos vetores.
```
De acordo com os resultados do correlograma, há uma correlação forte entre Sepal Lenght com Petal Lenght e Petal Width, assim como Petal Lenght e Petal Width. O único vetor que parece ter um comportamento significantemente distinto dos demais do ponto de vista linear é Sepal Width. No caso dessa variável, os índices de correlação são negativos com as demais e ela possuí uma correlação mais fraca.

Antes de realizar as estimativas, é preciso avaliar a dispersão dos dados para saber se é necessário padronizá-los para facilitar a interpretação dos componentes principais. Nesse caso, vamos criar um boxplot para analisar os dados:

```{r}
# criando o boxplot
boxplot(iris[,-5])
```
Embora a dispersão dos dados não seja tão grande, há uma diferença significativa na distribuição entre Sepal Lenght e Petal Width. Nesse sentido, iremos padronizar os dados para facilitar a interpretação dos coeficientes principais. Como a questão não solicita que os dados sejam separados em diferentes amostras - para teste e treino, vamos encontrar os componentes principais utilizando todo o conjunto de dados iris.
```{r}
set.seed(9845)
# Como a redução de dimensionalidade é feita apenas para as variáveis independentes,
# iremos remover a variável Species do cálculo.
# calculando os componentes principais
acp <- prcomp(iris[,-5],
              center = TRUE,
              scale. = TRUE)
# observação: as opções center e scale. servem para padronizar os dados.
# 1. Center centraliza os dados ao redor de zero
# 2. scale. torna a variância das variáveis unitária
# vejamos as estimativas dos componentes principais
acp
summary(acp)
```
Os resultados reportam 4 componentes principais. A primeira componente corresponde a aprox. 73% da variância total dos dados, enquanto a segunda corresponde a aproximadamente 23%. Em conjunto, os dois componentes respondem por aprox. 96% de toda a variabilidade das 4 variáveis, indicando que os demais seriam desnecessários, por explicarem uma parcela muito pequena dos dados. 

Vejamos o gráfico com os autovalores (variâncias) dos componentes principais:
```{r}
screeplot(acp, type = "lines")
```
Os autovalores são obtidos através do cálculo do quadrado dos coeficientes reportados como "standard deviation", anteriormente. Já a proporção da variância explicada por cada componente principal pode ser calculada através da razão
entre o autovalor do componente e o somatório dos autovalores de todos os componentes. Combinando os resultados do gráfico dos autovalores (usando o Teste Scree de Cattel (1966)) com os resultados presentes na tabela anterior, confirma-se que somente os 2 primeiros componentes são necessários para o modelo. Vejamos o gráfico que mostra a relevância de cada variável em relação
aos componentes:

```{r warning=FALSE}
# Para isso, usaremos o pacote ggfortify,
# que permite ao ggplot interpretar os coeficientes do ACP.
library(ggfortify)
# gráfico com os autovetores e os componentes principais.
autoplot(acp, data = iris, colour = "Species",
         loadings = TRUE, loadings.label = TRUE,
         scale = 0)
# Observações:
# 1. Loadings = TRUE determina que os autovetores devem ser reportados;
# 2. loadings.label = TRUE reporta o nome das variáveis ligadas ao vetor;
# 3. scale = 0 serve para remover a padronização dos autovetores.
```
O gráfico anterior possuí diversas características interessantes. Os valores projetados de cada vetor nos componentes principais determinam o seu nível de influência sobre aquele componente. No caso em questão, o componente principal 2
é determinado majoritariamente pelo comportamento de Sepal Width, enquanto o componente principal 1 apresenta pesos próximos para Petal Width, Lenght e Sepal Length. Além disso, o ângulo entre os vetores reportados mostra como essas variáveis são correlacionadas. Como é possível observar, Petal Lenght e Width são altamente correlacionadas e todas são pouco correlacionadas com Sepal Width.

Caso houvesse um ângulo de 90° graus entre os vetores, seria indicativo de que eles não são correlacionados. O mais próximo disso é a relação entre Sepal Lenght e Sepal Width.

Por fim, é importante destacar como o valor de cada um dos componentes principais é calculado. De acordo com os coeficientes reportados, o Componente principal 1 pode ser definido da seguinte maneira:

$$ CP_1 = 0,52 * Sepal.Lenght - 0,27 * Sepal.Width + 0,58*Petal.Lenght + 0,56 *
Petal.Width $$

O segundo componente segue a mesma lógica:

$$ CP_2 = -0,38 * Sepal.Lenght - 0,92 * Sepal.Width - 0,02*Petal.Lenght - 0,07 *
Petal.Width $$

Como os demais vetores explicam uma parcela insignificante da variabilidade e seguem a mesma lógica, não serão reportados.

# Questão 2

Realize análise fatorial para os dados do problema anterior.

------------------------------------------------------------------------

Assim como no exemplo da aula 14, para o caso da análise aplicada a **iris** não é possível realizar a análise fatorial considerando 2 fatores, pois o pacote **stats** não aceita valor superior a 1 para 4 variáveis:

```{r error=TRUE}
AF <- factanal(iris[,-5], factors = 2, rotation = "varimax")
```
Porém, a aplicação para apenas 1 fator não é problemática, dado que a escolha do número de fatores adequada usando a Regra de Kaiser-Guttman, em que se consideram apenas os fatores com autovalores maiores que 1, indica que o número de fatores adequado é 1, divergindo da análise gráfica através do Teste Scree:

```{r}
screeplot(acp, type = "lines")
abline(h=1)
```
Sendo assim, realizaremos as estimativas usando apenas um fator:

```{r}
set.seed(9845)
# Análise fatorial considerando apenas 1 fator
AF <- factanal(iris[,-5], factors = 1)
# Observações:
# 1. Como há apenas um fator, não há uma matriz de cargas fatoriais, mas apenas um vetor.
#    Assim, não é possível fazer nenhum tipo de rotação de fatores para simplificar
#    a interpretação.
AF
```
Há diversas informações pertinentes a serem consideradas:

1.  **Uniqueness** se refere aos ruídos do modelo. É a proporção da variabilidade de cada variável (a variância específica) que não pode ser explicada pelo único fator que criamos. Nota-se que o fator explica consideravelmente bem a variabilidade de Petal Lenght e Width, além de explicar grande parte da variabilidade de Sepal Lenght. No entanto, o fator contribui menos de 20% para a variância de Sepal Width. 

2.  **Loadings** se refere as cargas fatoriais. Esses valores indicam a importância do fator 1 na composição de cada uma das variáveis. Valores (em módulo) próximos de 1 indicam que o fator é muito relevante para explicar a variável. Já próximos a zero, baixa. Assim como adiantado no resultado sobre Uniqueness, as cargas fatoriais são consideravelmente elevadas para as variáveis Petal Lenght, Width e Sepal Lenght, indicando que elas são bem explicadas pelo fator 1. Já Sepal Width apresenta um
valor, em módulo, consideravelmente menor que os demais, indicando que ela não é bem explicada pelo fator 1. 

3.  **Comunalidade**: a comunalidade de cada variável não é reportada diretamente no output, mas pode ser calculada por duas
maneiras: (i) através da soma dos quadrados das cargas fatoriais de cada fator; e (ii) fazendo a conta: 1 - **Uniqueness** de cada variável. A comunalidade se refere a parcela da variância da variável que é explicada pelos fatores. No caso em questão, a comunalidade será:

```{r}
# cálculo de comunalidade
apply(AF$loadings^2,1,sum)
```
Como é possível perceber, o fator explica praticamente toda a variabilidade de Petal Lenght e Width e a maior parte de Sepal Lenght, mas explica apenas 18% de Sepal Width, indicando que não é apropriado para endereçar a variabilidade desta
variável. 

4.  **SS Loadings e Proportion of Var**: essa parte da tabela indica a proporção da variabilidade das variáveis explicadas por cada fator. Como há apenas um, não há a linha que reporta a variabilidade cumulativa. Os resultados indicam que o fator 1 explica aproximadamente 72% da variabilidade das variáveis.

-   SS Loadings é a soma dos quadrados das cargas fatoriais. Pode ser obtida através da conta:

```{r}
sum(AF$loadings^2)
```
5.  A última parte do output se refere a um teste de hipótese que avalia se o número de fatores no modelo é suficiente para capturar a dimensionalidade dos dados. Com o p-valor é próximo de zero, rejeitamos a hipótese nula, o que indica que o número de fatores do modelo é pequeno demais. Esse teste só é reportado porque as estimações dos parâmetros do modelo fatorial do pacote `stats` são feitas utilizando o método de máxima verossimilhança.

Podemos estimar as matrizes de covariâncias $\hat{\Sigma}$ e a residual através dos seguintes comandos:

```{r}
# matriz com Lambdas (cargas fatoriais)
Lambda <- AF$loadings
# matriz de ruídos
Psi <- diag(AF$uniquenesses)
# matriz de covariâncias amostral
S <- AF$correlation
# matriz de covariâncias estimada
Sigma <- Lambda %*% t(Lambda) + Psi
# Observação: t(Lambda) transpõe a matriz de cargas fatoriais
# vejamos a matriz de covariância estimada
Sigma
# matriz residual
mat_residual <- round(S - Sigma, 6)
mat_residual
```
Como é possível observar para a matriz residual, os valores que relacionam Sepal Width e Length não são próximos de zero, indicando que o modelo fatorial precisaria de um fator adicional para contemplar esta relação. Para as demais, o
modelo para ser adequado. Há a possibilidade de usar 2 fatores através do pacote `psych`, mas como será apresentado abaixo, a depender do método utilizado para estimação dos parâmetros, eles produzem casos ultra-Heywood (quando a comunalidade excede 1).

Um caso ultra-Heywood implica que um dos fatores únicos possuí uma variância negativa, que é um indicativo claro que algo está errado e as estimativas não são confiáveis. Abaixo segue um exemplo do resultado usando 2 fatores e o método de fatoração de minimização dos resíduos (default do pacote):

```{r message=FALSE, warning=FALSE}
library(psych)
```
```{r}
set.seed(9845)
# aplicação da análise com dois fatores usando minres
fa2_minres <- fa(iris[,-5], nfactors = 2, rotate = "varimax")
fa2_minres
```
Mesmo alterando a especificação do modelo com relação a forma com que os escores e cargas fatoriais são calculados o algorítmo continua chegando a uma solução do tipo Heywood:
```{r}
# aplicação da análise com dois fatores utilizando método de fator principal
fa2_pa <- fa(iris[,-5], nfactors = 2, rotate = "varimax", fm = "pa")
fa2_pa 
```
Como a convergência dos resultados é muito dependente do método aplicado ao utilizar 2 fatores (e com base no resultado do teste de Kaiser-Guttman), optou-se por realizar a análise com base em apenas um fator, assim como apresentado anteriormente.


# Questão 3

Obtenha as componentes independentes para os dados do Problema 1.

------------------------------------------------------------------------

A Análise de Componentes Independentes transforma um conjunto de vetores em um conjunto de componentes independentes e não gaussianos.

A partir do exercício 1, temos que são dois os principais componentes que maximizam a variação nas quatro variáveis do modelo são dois, como pode se observar no gráfico abaixo.

```{r}
screeplot(acp, type = "lines")
```

Nas tabelas abaixo, pode-se observar os coeficientes de componentes principais e a importância dos componentes. Em conjunto, os dois componentes respondem por aprox. 96% de toda a variabilidade das 4 variáveis.

```{r}
acp$rotation
summary(acp)
```

Dessa forma, como são dois componentes principais, vamos assumir que são 2 os componentes independentes. Para estimar os componentes independentes, vamos usar o pacote do R fastICA.

```{r}
# install.packages("fastICA")
library(fastICA)
# install.packages("ica")
library(ica)
ica_fast <- fastICA(iris[1:4],2)
```

A matrix A é:

```{r}
ica_fast$A
```

Isto é, 

$$ X_1 = `r paste(ica_fast$A[1,1])`S_1  `r paste( ica_fast$A[1,2])`S_2 +`r paste(ica_fast$A[1,3])`S_3 + `r paste(ica_fast$A[1,4])`S_4 $$
e
$$ X_2 = `r paste(ica_fast$A[2,1])`S_1  `r paste( ica_fast$A[2,2])`S_2  `r paste(ica_fast$A[2,3])`S_3  `r paste(ica_fast$A[2,4])`S_4 $$

A variancia que esses dois componentes independentes explicam é a mesma variância que os componentes, isto é, 96% aproximadamente:

```{r}
summary(acp)
```

Abaixo, podemos observar os gráficos dos dados pré-processados; dos componentes principais (que pode ser obitida pela matriz pré-branquamento (pre-whitening matrix), que projeta a matriz de pré-processados nos componentes principais); e por fim, um gráfico dos componentes independentes.

```{r}
plot(ica_fast$X, main = "Dados pré-processados")
plot(ica_fast$X %*% ica_fast$K, main = "Componentes do PCA")
plot(ica_fast$S, main = "Compontentes do ICA")
```


# Questão 4

Considere o conjunto de dados *Boston* do pacote *ISLR*, contendo 506 amostras e 14 variáveis. Escolha variáveis que você acha que são importantes para descrever os dados. Faça uma análise de CP e uma análise fatorial e tente interpretar as componentes e os fatores.

------------------------------------------------------------------------

Vamos extrair os dados e fazer uma análise preliminar
```{r}
# install.packages("ISLR")
library(ISLR)
library(MASS)
data("Boston")
boston <-  Boston
attach(boston)
head(boston)
glimpse(boston)
summary(boston)
```
São 14 variáveis no data set: 

CRIM - taxa de criminalidade per capita por cidade;

ZN - proporção de área residencial zoneada para lotes com mais de 25.000 pés quadrados;

INDUS - proporção de acres de negócios não comerciais por cidade;

CHAS - variável dummy do rio Charles (1 se o terreno faz fronteira com o rio; 0 caso contrário);

NOX - concentração de óxidos nítricos (partes por 10 milhões);

RM - número médio de quartos por habitação;

AGE - proporção de unidades ocupadas por proprietários construídas antes de 1940;

DIS - distâncias ponderadas para cinco centros de emprego de Boston;

RAD - índice de acessibilidade a rodovias radiais;

TAX - taxa de imposto sobre propriedade de valor total por US$ 10.000;

PTRATIO - proporção aluno-professor por cidade;

B - 1000(Bk - 0.63)^2 onde Bk é a proporção de pessoas negras por cidade;

LSTAT - % de status social mais baixo da população;

MEDV - Valor médio de casas ocupadas pelos proprietários em US$ 1.000.

São variáveis relacionadas ao mercado imobiliário: informações relacionads com crime, emprego, acessibilidade, caracerística dos imóveis estão disponíveis nessa base de dados. Provavelmente é uma base de dados para tentar explicar o preço dos imóveies, colocando medv como variável explicada.

Vamos prosseguir com a matrix de correlação entre as variáveias analisadas.

```{r}
# mudanças preliminares na base:
# exclusão do chas (variável binária)
# criando a correlação entre as variáveis
correlacao <- cor(boston , method = "pearson")
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# produzindo um gráfico para visualização
corrplot(correlacao, method = "color",
type = "upper", col = col(200),
addCoef.col = "black",
tl.col="black", tl.srt=45,
   number.cex=0.60)
```

Como podemos ver no correlograma acima, a variável chas é pouco correlacionada com todas as outras. Ela será excluída da análise e escolheremos todas as outras para realizar o PCA. Pelo correlograma, podemos ver que a maioria das variáveis não estão correlacionadas positivamente ou negativamente com cada uma das outras. Há, no entanto, pares que são correlacionados entre si, como por exemplo, lstat (% de status social mais baixo da população) e medv (Valor médio de casas ocupadas pelos proprietários em US$ 1.000).

Prosseguindo na análise: vamos avaliar a dispersão dos dados para verificar a necessidade de padronização e normalização.

```{r}
boston <- subset(boston, select = -chas)
boxplot(boston)
```

O gráfico indica a necessidade de padronização. Vamos assim, calcular os componentes principais:

```{r}
acp_boston <- prcomp(
  boston,
  center = TRUE,
  scale. = TRUE
)
acp_boston
summary(acp_boston)
screeplot(acp_boston, type = "lines")
```

De forma análoga ao exercício 1, os autovalores são obtidos através do cálculo do quadrado dos coeficientes reportados como “standard deviation”. Já a proporção da variância explicada por cada componente principal pode ser calculada através da razão entre o autovalor do componente e o somatório dos autovalores de todos os componentes. Combinando os resultados do gráfico dos autovalores com os resultados presentes na tabela anterior, confirma-se que somente os 3 primeiros componentes.

Agora, vamos processguir com a análise fatorial. Quantos são os faotres que precisamos estimar? vamos ustilizar a Regra de Kaiser-Guttman, como na questão 2.
O número de fatores adequados é 3, como pode ser anáilsado no Teste scree:

```{r}
screeplot(acp_boston, type = "lines")
abline(h=1)
```

Vamos prosseguir com a análise fatorial, nas especificações minres e fator principal.

```{r}
fa_boston_minres =  fa(boston, nfactors = 3, rotate = "varimax")

fa_boston_minres
```
```{r}
# install.packages("GPArotation")
library(GPArotation)

fa_boston_pa =  fa(boston, nfactors = 3, fm = "pa")
fa_boston_pa


```

Para interpretar esses três componentes, vamos analisar as tabelas de compontentes principais, cargas fatorias calculadas por minres e fator princpal, com os três componentes e as variáveis:

```{r}
acp_boston$rotation[,1:3]
fa_boston_minres$loadings
fa_boston_pa$loadings

```


Analisando os componentes individualmente:

PC1: Essa carga tem um valor relativamente alto para variáveis como indus, nox, age e lstat. Isso indica uma associação positiva entre essas variáveis e o componente principal 1. Portanto, **o PC1 pode estar capturando uma medida geral de desenvolvimento urbano ou condições socioeconômicas das áreas analisadas**.

PC2:
Essa carga tem valores altos para variáveis como zn, dis, rad e tax. Isso sugere uma associação positiva entre a proporção de área residencial zoneada para lotes grandes (zn), distâncias ponderadas para centros de emprego (dis), índice de acessibilidade a rodovias radiais (rad) e taxa de imposto sobre a propriedade (tax). **Portanto, o PC2 pode refletir um componente relacionado à localização, acessibilidade e características das áreas residenciais**.

PC3:
Essa carga tem valores altos para variáveis como crim, indus e lstat. Isso indica uma associação positiva entre a taxa de criminalidade per capita (crim), proporção de acres de negócios não comerciais por cidade (indus) e o percentual de status social mais baixo da população (lstat). **Portanto, o PC3 pode estar capturando um componente relacionado à criminalidade e características socioeconômicas associadas**.

Analisando os fatores calculados por fator princpal:

PA1:
Essa carga tem valores altos para variáveis como indus, nox, age e tax. Isso sugere uma associação positiva entre a proporção de acres de negócios não comerciais por cidade (indus), concentração de óxidos nítricos (nox), proporção de unidades ocupadas por proprietários construídas antes de 1940 (age) e taxa de imposto sobre a propriedade (tax). **Portanto, o PA1 pode estar relacionado a fatores socioeconômicos e ambientais que influenciam essas variáveis**.

PA2:
Essa carga tem valores altos para variáveis como rm, rad, ptratio e lstat. Isso indica uma associação positiva entre o número médio de quartos por habitação (rm), índice de acessibilidade a rodovias radiais (rad), proporção aluno-professor (ptratio) e o percentual de status social mais baixo da população (lstat).**Portanto, o PA2 pode refletir um componente relacionado à qualidade da habitação, acesso a serviços educacionais e características socioeconômicas**.

PA3:
Essa carga tem valores altos para variáveis como zn, rm, ptratio, black e medv. Isso sugere uma associação entre a proporção de área residencial zoneada para lotes grandes (zn), número médio de quartos por habitação (rm), proporção aluno-professor (ptratio), proporção de pessoas negras por cidade (black) e o valor médio das casas ocupadas pelos proprietários (medv).**Portanto, o PA3 pode estar relacionado a fatores relacionados à qualidade da habitação, diversidade racial e valor das propriedades**.

Analisando os fatores calculados por Minres:

MR1:
Essa carga tem valores altos para variáveis como indus, nox, age, dis, tax e lstat. Isso sugere uma associação positiva entre a proporção de acres de negócios não comerciais por cidade (indus), concentração de óxidos nítricos (nox), proporção de unidades ocupadas por proprietários construídas antes de 1940 (age), distâncias ponderadas para centros de emprego (dis), taxa de imposto sobre a propriedade (tax) e o percentual de status social mais baixo da população (lstat).**Portanto, o MR1 pode estar capturando fatores socioeconômicos e ambientais comuns a essas variáveis**.

MR2:
Essa carga tem valores altos para variáveis como indus, nox, age e lstat. Isso indica uma associação positiva entre a proporção de acres de negócios não comerciais por cidade (indus), concentração de óxidos nítricos (nox), proporção de unidades ocupadas por proprietários construídas antes de 1940 (age) e o percentual de status social mais baixo da população (lstat). **Portanto, o MR2 pode estar relacionado a fatores socioeconômicos e ambientais específicos a essas variáveis**.

MR3:
Essa carga tem valores altos para variáveis como zn, indus, nox, age, rad, tax e lstat. Isso sugere uma associação entre a proporção de área residencial zoneada para lotes grandes (zn), proporção de acres de negócios não comerciais por cidade (indus), concentração de óxidos nítricos (nox), proporção de unidades ocupadas por proprietários construídas antes de 1940 (age), índice de acessibilidade a rodovias radiais (rad), taxa de imposto sobre a propriedade (tax) e o percentual de status social mais baixo da população (lstat).**Portanto, o MR3 pode estar relacionado a uma combinação de fatores socioeconômicos e ambientais que afetam essas variáveis**.
